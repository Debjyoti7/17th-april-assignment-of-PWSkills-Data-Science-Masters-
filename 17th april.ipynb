{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75030976-f34a-4ac9-9274-0b4356af1d47",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa41c3-2860-4f1c-b0ca-514c9bd2c2ed",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression (GBR) is a popular machine learning algorithm used for regression problems, which involves predicting a continuous numerical value. It works by building an ensemble of weak decision trees, where each tree is trained to correct the errors made by the previous tree in the ensemble. The algorithm works by iteratively adding decision trees to the ensemble, with each subsequent tree attempting to minimize the residual errors made by the previous tree. This is done by using a gradient descent optimization technique to find the direction of steepest descent of the loss function, which measures the difference between the predicted and actual values of the target variable.\n",
    "## In GBR, the final prediction is the sum of the predictions made by each individual tree in the ensemble. The strength of GBR lies in its ability to capture complex non-linear relationships between the input features and the target variable, while also avoiding overfitting by controlling the complexity of the model through regularization parameters. It is widely used in various fields such as finance, engineering, and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512fbb7-6ac4-4569-9f98-bb5e12099578",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355f0bca-4717-46c8-9a63-44334fa70391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 131.40423550937257\n",
      "R-squared score: 0.0929049451916577\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=25)\n",
    "\n",
    "# Initialize the residuals to be the same as the target values\n",
    "residuals = y.copy()\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals to be the same as the target values\n",
    "        residuals = y.copy()\n",
    "\n",
    "        # Fit a sequence of trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Fit a regression tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=1)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Add the tree to the ensemble\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update the residuals by subtracting the prediction from the tree\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions from all the trees in the ensemble\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Train a gradient boosting regressor on the training set\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance on the testing set\n",
    "y_pred = gbr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared score:\", r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d50d22-5e9f-452d-956f-dcf5b73e2761",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096ae396-8e13-4825-9e54-5bff3856695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 150}\n",
      "Best mean squared error: -0.8987672238366354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a gradient boosting regressor object\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(gbr, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean squared error\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best mean squared error:\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5c57b-dce2-4229-befd-830fd28eb60e",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfed0f-ddce-4746-aa6a-f75113a4ce0e",
   "metadata": {},
   "source": [
    "## In Gradient Boosting, a weak learner is a simple or relatively low-complexity model that is used as a building block to create a stronger, more complex model. A weak learner is typically a decision tree with a small number of nodes or a linear regression model. In the context of Gradient Boosting, the weak learner is used to create an ensemble of models, where each model is trained to correct the errors made by the previous model in the ensemble. The idea is that by combining a sequence of weak learners in a specific way, the ensemble model can achieve high accuracy on the prediction task. The iterative process of training weak learners and combining them in an ensemble is known as boosting. In each iteration of the boosting process, the weak learner is trained on the residual errors of the previous iteration, with the goal of minimizing these errors in the subsequent iteration. This process continues until the desired level of accuracy is achieved or until a predefined stopping criterion is met.\n",
    "## The use of weak learners in Gradient Boosting has several advantages, including reducing the risk of overfitting, improving the generalization performance of the model, and being able to handle a wide range of data types and structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63404e-51fc-40cb-9cef-fb01d51bd6af",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a93b3-b15f-44ec-9494-bdf202663a04",
   "metadata": {},
   "source": [
    "## The intuition behind the Gradient Boosting algorithm is to combine a sequence of weak learners in a specific way to create a strong learner that can make accurate predictions on a given task. The algorithm works in an iterative manner, where at each iteration a weak learner is trained on the errors made by the previous learner. Specifically, the algorithm fits a weak learner to the residuals of the current prediction, which is the difference between the predicted values and the actual values. In each iteration, the algorithm tries to find the best weak learner to add to the ensemble by minimizing a loss function, which measures the difference between the predicted values and the actual values. The loss function used in Gradient Boosting is typically the mean squared error or the log loss, depending on the type of problem being solved. Once the weak learner is added to the ensemble, its predictions are combined with the predictions of the previous learners using a weighting scheme that gives more weight to the more accurate learners. This way, the algorithm gradually learns to correct the errors made by the previous learners, which leads to a more accurate and robust model.\n",
    "## Overall, the intuition behind Gradient Boosting is to build an ensemble of weak learners that can learn from their mistakes and gradually improve their predictions, resulting in a strong learner that can accurately predict the output values for a given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1516b5-8717-4c3b-8b43-03ccca5aa692",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b1d72-9491-49cf-bcf3-3c45cff0eb14",
   "metadata": {},
   "source": [
    "## The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative and additive manner. At each iteration, the algorithm fits a weak learner to the residuals of the current prediction, which is the difference between the predicted values and the actual values. This weak learner is then added to the ensemble with a certain weight.\n",
    "## The process of adding the weak learner to the ensemble is done in a way that minimizes a loss function, which measures the difference between the predicted values and the actual values. The loss function used in Gradient Boosting is typically the mean squared error or the log loss, depending on the type of problem being solved. After the first weak learner is added to the ensemble, the algorithm evaluates the performance of the ensemble on the training data and calculates the residuals for each training example. The next weak learner is then trained on these residuals, with the goal of reducing the remaining error in the prediction. This process continues iteratively, with each weak learner added to the ensemble improving the accuracy of the predictions. To prevent overfitting, a technique called \"shrinkage\" is often used, which involves reducing the contribution of each weak learner by a small factor (the learning rate) before adding it to the ensemble. This helps to prevent the algorithm from fitting the training data too closely and improves the generalization performance of the model.\n",
    "## Overall, the Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding them to the ensemble in a way that minimizes the loss function and reduces the residual errors in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999dd04f-9487-4214-a0c5-201a155931be",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb811c4-38db-47d4-b576-8e88f26bd434",
   "metadata": {},
   "source": [
    "## The mathematical intuition behind the Gradient Boosting algorithm involves the following steps: 1. Define the loss function: The first step in constructing the mathematical intuition of Gradient Boosting is to define a loss function that measures the difference between the predicted values and the actual values. The loss function used in Gradient Boosting is typically the mean squared error or the log loss, depending on the type of problem being solved.\n",
    "## 2. Initialize the prediction: The next step is to initialize the prediction with a constant value, which is typically the mean of the target variable. This is the first weak learner in the ensemble.\n",
    "## 3. Iterate over the weak learners: In each iteration, a new weak learner is added to the ensemble by fitting it to the residual errors of the current prediction. The residual errors are calculated as the difference between the predicted values and the actual values.\n",
    "## 4. Optimize the loss function: The weak learner is trained to optimize the loss function by finding the parameters that minimize the loss on the training data.\n",
    "## 5. Update the prediction: Once the weak learner is trained, its predictions are combined with the predictions of the previous learners using a weighting scheme that gives more weight to the more accurate learners. This way, the algorithm gradually learns to correct the errors made by the previous learners, which leads to a more accurate and robust model.\n",
    "## 6. Stop the algorithm: The algorithm continues iterating over the weak learners until a predefined stopping criterion is met, such as reaching a maximum number of iterations or a minimum improvement in the loss function.\n",
    "## Overall, the mathematical intuition behind the Gradient Boosting algorithm involves iteratively adding weak learners to the ensemble and training them to optimize the loss function by minimizing the residual errors in the prediction. The resulting ensemble of weak learners can accurately predict the output values for a given input.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c6771-1b1c-4d15-9018-2549ac77d933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
